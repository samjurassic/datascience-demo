{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMPShBCP7PX470+RzMYzt0G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/samjurassic/datascience-demo/blob/main/coda/HBS_CoDA_Python_Part1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# HBS CoDA Python Workshop - Part 1\n",
        "Learning objectives include:\n",
        "\n",
        "• Basic data types in Python\n",
        "\n",
        "• Load a CSV or Excel file into Python\n",
        "\n",
        "• Conduct exploratory data analysis with Pandas and Seaborn\n",
        "\n",
        "• Calculating summary statistics\n",
        "\n",
        "• Using groupby to aggregate data\n",
        "\n",
        "• Creating scatter, bar, line, and box plots\n",
        "\n",
        "• Decision tree learning with sk-learn\n",
        "\n",
        "• Analyze multiple files with DuckDB\n",
        "\n",
        "• Vibe-code a data pipeline using genAI"
      ],
      "metadata": {
        "id": "yqbIfu9Q-Mnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Python Basics"
      ],
      "metadata": {
        "id": "LC260AfZRqsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Types and DATA STRUCTURES ---\n",
        "\n",
        "# string (in quotes)\n",
        "my_name = \"Sam\"\n",
        "\n",
        "# note: print with a f-string to interpolate values in {}\n",
        "print(f\"My name is {my_name}\")\n",
        "\n",
        "# integer\n",
        "favorite_number = 7\n",
        "\n",
        "# float\n",
        "sales_tax = 0.0875\n",
        "\n",
        "# boolean\n",
        "it = True\n",
        "\n",
        "# list\n",
        "prices = [10, 214, 150, 59]\n",
        "\n",
        "# PYTHON INDICES START AT ZERO, access with integer in sq. brackets []\n",
        "print(f\"First price: {prices[0]}\")\n",
        "\n",
        "# dict: Used for named key: value pairs, often a row of data (in excel terms)\n",
        "product_row = {\"id\": 101, \"name\": \"Keyboard\", \"price\": 25, \"in_stock\": True}\n",
        "\n",
        "# Accessing dictionary data by name (Key)\n",
        "print(f\"{product_row[\"name\"]} Price: {product_row[\"price\"]}\")\n"
      ],
      "metadata": {
        "id": "CFwTVYpBRu2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FUNCTIONS ---\n",
        "\n",
        "# A Function acting as a reusable tool\n",
        "# Includes Control Flow (Logic) to make decisions\n",
        "\n",
        "def calculate_tax(price):\n",
        "    if price >= 100.0:\n",
        "        return price * 1.0875  # High tax for luxury items\n",
        "    else:\n",
        "        return price * 1.00  # Low tax for cheap items\n",
        "\n",
        "\n",
        "# --- LOOPS/ITERABLES ---\n",
        "\n",
        "# The For loop\n",
        "taxed_prices = [] # start with empty list\n",
        "\n",
        "for p in prices:\n",
        "    new_price = calculate_tax(p) # call function\n",
        "    taxed_prices.append(new_price) # list.append() adds item to list\n",
        "\n",
        "print(f\"For Loop Result: \\t{taxed_prices}\")\n",
        "\n",
        "# The \"Pythonic Way\" (List Comprehension)\n",
        "# A loop condensed into one line: [ ACTION for ITEM in LIST ]\n",
        "\n",
        "taxed_prices_v2 = [calculate_tax(p) for p in prices]\n",
        "\n",
        "print(f\"Comprehension Result: \\t{taxed_prices_v2}\")\n",
        "\n",
        "# note: try to balance code readability and length"
      ],
      "metadata": {
        "id": "II_Y2JomOZbK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrames 101"
      ],
      "metadata": {
        "id": "At2SIpq2RV1-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0cyXNlgEGPs"
      },
      "outputs": [],
      "source": [
        "# imports bring in code that isn't included in base python\n",
        "\n",
        "import pandas as pd # dataframes\n",
        "import numpy as np # numerical calculations\n",
        "\n",
        "# plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remember Dictionaries? A DataFrame is just a \"Dictionary of Lists\"\n",
        "# Each Key is a Column Name. Each Value is the List of data.\n",
        "data = {\n",
        "    \"Product\": [\"Apple\", \"Banana\", \"Orange\"],\n",
        "    \"Price\": [1.20, 0.50, 2.50],\n",
        "    \"In_Stock\": [True, True, False]\n",
        "}\n",
        "\n",
        "# Convert the dictionary to a DataFrame\n",
        "tiny_df = pd.DataFrame(data)\n",
        "print(\"--- Tiny DataFrame ---\")\n",
        "print(tiny_df)\n",
        "print(\"\\n\")"
      ],
      "metadata": {
        "id": "xnQey2XhYONj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering data\n",
        "\n",
        "# query (returns a copy)\n",
        "tiny_df.query(\"Product == 'Apple'\")"
      ],
      "metadata": {
        "id": "jRP_NLn7To_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# .loc returns original object\n",
        "tiny_df.loc[tiny_df[\"Product\"] == \"Apple\"]"
      ],
      "metadata": {
        "id": "AX2fbdkwTxPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assignment\n",
        "\n",
        "print(\"Before update\", tiny_df, sep=\"\\n\")\n",
        "\n",
        "# update existing values with .loc\n",
        "tiny_df.loc[tiny_df[\"Product\"] == \"Apple\", \"Price\"] = 1.45\n",
        "\n",
        "# assign new column\n",
        "tiny_df[\"Date\"] = \"2026-02-18\"\n",
        "\n",
        "print(\"\", \"After update\", tiny_df, sep=\"\\n\")"
      ],
      "metadata": {
        "id": "gkNBgDyrT_Dg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading a CSV from a URL and Data Cleanup\n",
        "\n",
        "We will be using food import data from the USDA. This file includes several common data issues we will address using Pandas:\n",
        "- Mixed data types in the same column\n",
        "- Mixed units in the same column\n",
        "- Aggregate data included with disaggregated data"
      ],
      "metadata": {
        "id": "hxClurKwPMqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://www.ers.usda.gov/data-products/us-food-imports/documentation\n",
        "\n",
        "link = \"https://www.ers.usda.gov/media/6495/summary-data-on-annual-food-imports-values-and-volume-by-food-category-and-source-country.csv?v=37251\"\n",
        "\n",
        "# encoding is usually figured out automatically, but if you get an encoding error you can specify\n",
        "food_imports = pd.read_csv(link, encoding=\"cp1252\")\n",
        "\n",
        "# food_imports.to_csv(\"annual_food_imports.csv\", index=False)"
      ],
      "metadata": {
        "id": "-7zctmc_EJHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food_imports.head()"
      ],
      "metadata": {
        "id": "05TFpsU9FQwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.info() gives size, type, null information\n",
        "food_imports.info()"
      ],
      "metadata": {
        "id": "tohWGppCV5E6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food_imports.describe().round(2)"
      ],
      "metadata": {
        "id": "h5W1Wm1PVEkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# year is not numeric... let's try to re-assign to integer\n",
        "try:\n",
        "  food_imports[\"Year\"] = food_imports[\"YearNum\"].astype(int)\n",
        "except Exception as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "id": "-0in4UphV1uV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food_imports[\"YearNum\"].value_counts().tail(5)"
      ],
      "metadata": {
        "id": "gudiDQhAXO_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# what that means\n",
        "food_imports.query(\"YearNum.isin(['means10years', 'means'])\").head(5)"
      ],
      "metadata": {
        "id": "rAY80kfiWsm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's drop these rows with df.drop\n",
        "rows_to_drop = food_imports.query(\"YearNum.isin(['means10years', 'means'])\").index\n",
        "food_imports.drop(rows_to_drop, axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "C8M0Em2MW0O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now we can make a new integer year column\n",
        "food_imports[\"Year\"] = food_imports[\"YearNum\"].astype(int)\n",
        "\n",
        "food_imports.describe()"
      ],
      "metadata": {
        "id": "4Ip7exutXHQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Line plot example"
      ],
      "metadata": {
        "id": "f7m_jgSQxGqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can chain multiple methods together\n",
        "vegetables_annual_usd = (food_imports\n",
        "                         .query(\"Commodity == 'Total vegetables and preparations' and UOM == 'Million $'\")\n",
        "                         .groupby([\"Country\", \"Year\"])[\"FoodValue\"]\n",
        "                         .sum()\n",
        "                         .reset_index())\n",
        "\n",
        "plot_data = vegetables_annual_usd[~vegetables_annual_usd['Country'].isin(['WORLD', 'REST OF WORLD', 'WORLD (Quantity)'])].copy()"
      ],
      "metadata": {
        "id": "df6veYibIeX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set figure size to make it wider/readable\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# CREATE PLOT\n",
        "sns.lineplot(\n",
        "    data=plot_data,\n",
        "    x=\"Year\",\n",
        "    y=\"FoodValue\",\n",
        "    hue=\"Country\",\n",
        "    legend=\"auto\",\n",
        "    linewidth=2\n",
        ")\n",
        "\n",
        "# log y axis (don't need to transform data in dataframe to do this)\n",
        "plt.yscale('log')\n",
        "\n",
        "# FORMATTING\n",
        "plt.title(\"Vegetable Food Value by Country\", fontsize=16)\n",
        "plt.ylabel(\"Millions of USD\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ntbMJ9oII_BJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "food_imports.Commodity.value_counts().to_frame()[0:9]"
      ],
      "metadata": {
        "id": "o0Q94gK2oa9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Boxplot example"
      ],
      "metadata": {
        "id": "4tslelXavbiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coffee_imports = food_imports[~food_imports['Country'].isin(['WORLD', 'REST OF WORLD', 'WORLD (Quantity)'])].query(\"Commodity == 'Coffee beans, unroasted'\")"
      ],
      "metadata": {
        "id": "PER7rpHinXhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "\n",
        "sns.boxplot(\n",
        "    data=coffee_imports,\n",
        "    x=\"Country\",\n",
        "    y=\"FoodValue\",\n",
        "    legend=\"auto\",\n",
        "    hue=\"Country\"\n",
        ")\n",
        "\n",
        "# Rotate labels so they don't overlap\n",
        "plt.title(\"Range of Unroasted Coffee Imports by Country 1999-2024\")\n",
        "plt.xlabel(\"Country of Origin\")\n",
        "plt.ylabel(\"Millions of USD\")\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "7gp2qSzVmWfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bar plot Example"
      ],
      "metadata": {
        "id": "P1OXpz22mgLd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "swiss_imports = food_imports.query(\"Country == 'SWITZERLAND'\")\n",
        "\n",
        "swiss_imports_sum = swiss_imports.groupby(\"Commodity\")[\"FoodValue\"].sum().reset_index()\n",
        "\n",
        "sns.barplot(swiss_imports_sum, x=\"Commodity\", y=\"FoodValue\")"
      ],
      "metadata": {
        "id": "t5Q1ihtaZpmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swiss_imports_sum"
      ],
      "metadata": {
        "id": "h4YXWRAqbCLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate difference using row indices 5 and 2\n",
        "# tea_and_spices = (\n",
        "total_coffee_tea_spices = swiss_imports_sum.loc[swiss_imports_sum[\"Commodity\"] == \"Total coffee, tea, and spices\", \"FoodValue\"].values[0]\n",
        "total_coffee = swiss_imports_sum.loc[swiss_imports_sum[\"Commodity\"] == \"Coffee, roasted and instant\", \"FoodValue\"].values[0]\n",
        "\n",
        "tea_and_spices = round(total_coffee_tea_spices - total_coffee, 2)\n",
        "print(tea_and_spices)\n",
        "\n",
        "# Add the new row at index 6\n",
        "swiss_imports_sum.loc[6] = ['Tea and spices', tea_and_spices]"
      ],
      "metadata": {
        "id": "dA-bbFytbj9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swiss_imports_sum.drop(swiss_imports_sum[swiss_imports_sum['Commodity'] == 'Total coffee, tea, and spices'].index, inplace=True)"
      ],
      "metadata": {
        "id": "UnfqKLxfdCOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "swiss_imports_sum"
      ],
      "metadata": {
        "id": "unxUdSENb9TL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# add color to bars (labels still look bad)\n",
        "sns.barplot(swiss_imports_sum, x=\"Commodity\", y=\"FoodValue\", hue=\"Commodity\")"
      ],
      "metadata": {
        "id": "ZfdwL2lzdlVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the dataframe by value (descending)\n",
        "swiss_imports_sum = swiss_imports_sum.sort_values(\"FoodValue\", ascending=False)\n",
        "\n",
        "# Create the plot\n",
        "plt.figure(figsize=(10, 6)) # Make it wider so labels fit\n",
        "ax = sns.barplot(\n",
        "    data=swiss_imports_sum,\n",
        "    x=\"Commodity\",\n",
        "    y=\"FoodValue\",\n",
        "    hue=\"Commodity\",\n",
        "    palette=\"viridis\",  # Options: \"magma\", \"rocket\", \"flare\", \"crest\"\n",
        "    legend=False        # Removes redundant legend since x-axis has labels\n",
        ")\n",
        "# Add labels on top of the bars\n",
        "# 'fmt=\"%.1f\"' keeps one decimal place as seen in your original data\n",
        "for container in ax.containers:\n",
        "    ax.bar_label(container, padding=3, fmt='%.1f')\n",
        "\n",
        "\n",
        "# increase room for bar labels on y axis\n",
        "max_val = swiss_imports_sum['FoodValue'].max()\n",
        "ax.set_ylim(0, max_val * 1.1)\n",
        "\n",
        "# Rotate labels so they don't overlap\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.title(\"Swiss Food Imports by Value 1999-2024\")\n",
        "plt.ylabel(\"Millions of USD\")\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "94Ff8kbCdzYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Learning: Decision Tree Example"
      ],
      "metadata": {
        "id": "MyqcuwhKF05c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load and clean data\n",
        "cars = sns.load_dataset(\"mpg\").dropna()\n",
        "cars.head()"
      ],
      "metadata": {
        "id": "_4F4ndR4xkY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SCATTER PLOT\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(data=cars, x=\"horsepower\", y=\"mpg\", hue=\"origin\", palette=\"viridis\")\n",
        "plt.title(\"Cars: Horsepower vs. MPG\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k3aVIXkbXgRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PREPARE DATA\n",
        "# Features (X) and Target (y)\n",
        "X = cars[['horsepower', 'mpg', 'weight']]\n",
        "y = cars['origin']\n",
        "\n",
        "# Split into Training (80%) and Testing (20%) sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# TRAIN THE MODEL\n",
        "# We set 'max_depth=3' so the tree isn't too big to look at!\n",
        "clf = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model Training Complete.\")\n",
        "print(f\"Model Accuracy on test data: {clf.score(X_test, y_test):.2%}\")"
      ],
      "metadata": {
        "id": "4UubNCG1F8XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PLOT THE TREE\n",
        "plt.figure(figsize=(16, 10))\n",
        "plot_tree(\n",
        "    clf,\n",
        "    feature_names=X.columns,\n",
        "    class_names=clf.classes_,\n",
        "    filled=True,      # Colors the boxes based on the majority class\n",
        "    rounded=True,     # Makes boxes look nicer\n",
        "    fontsize=12\n",
        ")\n",
        "plt.title(\"Decision Tree: How the computer predicts Car Origin\")\n",
        "plt.show()\n",
        "\n",
        "# note: gini impurity is used as the tree's objective function\n",
        "# this measures node impurity i.e. how many incorrect examples are in the node\n",
        "# 0 is perfect classification, 1 - 1/N(classes) is worst (here 1-1/3 = 0.667)"
      ],
      "metadata": {
        "id": "OhYT6DSGGI6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm, display_labels=clf.classes_).plot(cmap='Blues')\n",
        "\n",
        "# correct predictions are along the Top Left-Bottom Right diagonal\n",
        "\n",
        "print(classification_report(y_test, y_pred, target_names=clf.classes_))\n",
        "\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html\n",
        "\n",
        "print(\"\"\"\n",
        "Precision is the ratio tp / (tp + fp) where tp is the number of true positives and fp the number of false positives.\n",
        "Precision is intuitively the ability of the classifier not to label a negative sample as positive.\n",
        "\n",
        "Recall is the ratio tp / (tp + fn) where tp is the number of true positives and fn the number of false negatives.\n",
        "Recall is intuitively the ability of the classifier to find all the positive samples.\n",
        "\n",
        "F-1 score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0.\n",
        "The F-beta score weights recall more than precision by a factor of beta. beta == 1.0 means recall and precision are equally important.\n",
        "\n",
        "The support is the number of occurrences of each class in y_true.\n",
        "\n",
        "Macro average (averaging the unweighted mean per label)\n",
        "Weighted average (averaging the support-weighted mean per label)\n",
        "\"\"\")\n"
      ],
      "metadata": {
        "id": "xilYdv23Jik_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling multiple files/data pipeline vibecoding"
      ],
      "metadata": {
        "id": "tSTY49xxKR0t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import duckdb # read more at https://duckdb.org/\n",
        "\n",
        "# 1. Generate synthetic data\n",
        "countries = ['BRAZIL', 'COLOMBIA', 'VIETNAM', 'PERU', 'GUATEMALA']\n",
        "data = {\n",
        "    'Country': np.random.choice(countries, 500),\n",
        "    'FoodValue': np.random.normal(1500, 800, 500),\n",
        "    'Date': pd.date_range(start='2020-01-01', periods=500, freq='D')\n",
        "}\n",
        "df_raw = pd.DataFrame(data)\n",
        "\n",
        "# 2. Split by Country and by 100-row chunks\n",
        "os.makedirs('raw_imports', exist_ok=True)\n",
        "\n",
        "for country, group in df_raw.groupby('Country'):\n",
        "    # Split this country's data into chunks of 100\n",
        "    chunks = [group[i:i+100] for i in range(0, group.shape[0], 100)]\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        filename = f\"raw_imports/import_{country}_{i}.csv\"\n",
        "        chunk.to_csv(filename, index=False)\n",
        "\n",
        "print(f\"Created {len(os.listdir('raw_imports'))} files in /raw_imports/\")"
      ],
      "metadata": {
        "id": "TDOyHkSxKYhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can run this on all files, or let DuckDB handle it via a View.\n",
        "# Let's use DuckDB for the \"Heavy Lifting\" because it's cooler.\n",
        "\n",
        "# Create connection\n",
        "con = duckdb.connect()\n",
        "\n",
        "# 1. Read all files\n",
        "# 2. Clean/Standardize on the fly\n",
        "# 3. Group by Country\n",
        "pipeline_sql = \"\"\"\n",
        "    CREATE OR REPLACE TABLE clean_imports AS\n",
        "    SELECT\n",
        "        upper(Country) as Country,\n",
        "        AVG(FoodValue) as Avg_Value,\n",
        "        SUM(FoodValue) as Total_Value,\n",
        "        COUNT(*) as Transaction_Count,\n",
        "        max(Date) as Latest_Date\n",
        "    FROM read_csv_auto('raw_imports/*.csv')\n",
        "    GROUP BY Country\n",
        "    HAVING Total_Value > 0\n",
        "    ORDER BY Total_Value DESC\n",
        "\"\"\"\n",
        "\n",
        "con.execute(pipeline_sql)\n",
        "print(\"Transformation Pipeline Complete.\")\n",
        "\n",
        "# Final check: Peek at the results\n",
        "print(con.execute(\"SELECT * FROM clean_imports\").df())"
      ],
      "metadata": {
        "id": "YMuxAl6YSwOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write to Parquet (The industry standard for ML data)\n",
        "con.execute(\"COPY clean_imports TO 'final_summary.parquet' (FORMAT PARQUET)\")\n",
        "\n",
        "# Write to CSV (For your boss to open in Excel)\n",
        "con.execute(\"COPY clean_imports TO 'final_summary.csv' (HEADER, DELIMITER ',')\")\n",
        "\n",
        "con.close()"
      ],
      "metadata": {
        "id": "CnMt5-cYTfxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template for data pipeline\n",
        "\n",
        "Role: Act as a Data Engineer. Produce Python code for the following ETL pipeline.\n",
        "\n",
        "1. SOURCE (Extract):\n",
        "Look for files in [FOLDER_NAME] with the extension [CSV/PARQUET].\n",
        "Handle multiple files using [glob/duckdb/pathlib].\n",
        "\n",
        "2. CLEANING (Transform - Python):\n",
        "Apply these rules to every file: [e.g., strip whitespace, convert dates, handle nulls].\n",
        "Remove rows where [CONDITION].\n",
        "\n",
        "3. AGGREGATION (Transform - SQL/DuckDB):\n",
        "Create a virtual view of the cleaned data.\n",
        "Run a SQL query to: [e.g., Group by X, Sum Y, Join with Z].\n",
        "\n",
        "4. SINK (Load):\n",
        "Write the final output to [DIRECTORY/FILENAME] in [PARQUET/CSV] format."
      ],
      "metadata": {
        "id": "j7-MLCDNTtB4"
      }
    }
  ]
}