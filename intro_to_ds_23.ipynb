{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to Data Science"
      ],
      "metadata": {
        "id": "0t3zUz6Suy2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas Basics\n",
        "\n",
        "Pandas is the most popular data analysis library for Python. It's inspired by earlier features of SQL and R, but has continued to progress and add support for the latest hardware technologies (parallel, in-memory, cloud, ...) as well as advanced analysis capabilities.\n",
        "\n",
        "The fundamental object we'll be using is the DataFrame. This is basically just a table, but with a lot of built-in, powerful data analysis methods."
      ],
      "metadata": {
        "id": "MpAo-MZVuy2F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "wJP305V3uy2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataFrames and Series\n",
        "\n",
        "DataFrames are the table-like type used to store data in Pandas. Series are single columns of data - each column of a DataFrame is a series. You can make a series independently from a DataFrame, for example if you have a list and want to call some analysis methods on it."
      ],
      "metadata": {
        "id": "rfihvQ3euy2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groceries = {\"item\": [\"bananas\", \"apples\", \"oranges\"], \"quantity\": [4, 2, 8]}\n",
        "\n",
        "groceries_df = pd.DataFrame(groceries)\n",
        "\n",
        "print(\"Dict:\\n{}\\n\".format(groceries))\n",
        "print(\"DataFrame:\\n{}\".format(groceries_df))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "3y0trdoauy2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prices = pd.Series([3.25, 4.50, 1.75])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "WQjM-A6wuy2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can assign new columns to a DataFrame by writing:\n",
        "`df[\"new_column\"] = some_data`"
      ],
      "metadata": {
        "id": "MnaaiBGluy2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groceries_df[\"price\"]= prices"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "jcmI4Wnguy2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`df.head()` prints the first 6 rows of the DataFrame, `df.tail()` prints the last 6. You can also pass a number of rows, like `df.head(10)` to display a custom number of rows."
      ],
      "metadata": {
        "id": "maJUT06ouy2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groceries_df.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "WbgGJM_Duy2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# add a subtotal column\n",
        "groceries_df[\"subtotal\"] = groceries_df.quantity * groceries_df.price"
      ],
      "metadata": {
        "id": "ncAfM5ZviRc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groceries_df.head()"
      ],
      "metadata": {
        "id": "tWsZfckviaj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Selecting Data"
      ],
      "metadata": {
        "id": "fDWOpgFAuy2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# select by column name OR attribute groceries_df.item\n",
        "groceries_df[\"item\"]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "LNXbUVvPuy2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df.loc is used for label-based indexing\n",
        "groceries_df.loc[1,\"price\"]"
      ],
      "metadata": {
        "id": "IL4wb9WFg3NQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.iloc is used for integer-based indexing\n",
        "groceries_df.iloc[1, 2]"
      ],
      "metadata": {
        "id": "6pGjrxY-hYdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can select column ranges of data by passing a list of columns\n",
        "groceries_df[[\"item\", \"price\"]]"
      ],
      "metadata": {
        "id": "O8fVeFKvhh7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you can select rows the same way using loc or iloc\n",
        "groceries_df.loc[[0,1],]"
      ],
      "metadata": {
        "id": "UvyflB9Eh9YR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reshaping Data\n",
        "\n",
        "Datasets are not always organized the way we want them to be - sometimes we need each row to have a single data point, other times we might want each row to contain multiple data points. This might be for making a plot or producing statistics or a model.\n",
        "\n",
        "Pandas uses the following concepts to describe dataset layout:\n",
        "- Index: columns/id to identify a row\n",
        "- Columns: named columns per row\n",
        "- Values: measurements/data we want to use\n",
        "\n",
        "Usually, datasets come in one of two layouts:\n",
        "- Long: one measurement per row, includes measurement description as a column\n",
        "- Wide: many measurements per row\n",
        "\n",
        "Pandas indexing can make reshaping complicated - read more here https://pandas.pydata.org/docs/user_guide/reshaping.html#reshaping-pivot"
      ],
      "metadata": {
        "id": "JFWkFu8cJhTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# melting is used to make data longer\n",
        "groceries_df.melt(id_vars=[\"item\"])"
      ],
      "metadata": {
        "id": "xd-R_zK4QOB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pivot_table can be used to make data wider\n",
        "\n",
        "# let's save the melted data\n",
        "groceries_melted = groceries_df.melt(id_vars=[\"item\"])\n",
        "\n",
        "# use pivot_table to get the data back in the original shape (reset_index() makes item a column instead of an index here)\n",
        "groceries_melted.pivot_table(index=\"item\", columns=\"variable\", values=\"value\").reset_index()"
      ],
      "metadata": {
        "id": "4IeCnu7UjU5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Types of data\n",
        "\n",
        "You might be familiar with types from software engineering - how information is represented and encoded. In data science, it's important to know what kind of data types we have because only some types of data can be used for certain types of analysis. There are 4 main categories of data:\n",
        "\n",
        "Quantitative data\n",
        "- **Continuous**, a real number, e.g. temperature, height, \n",
        "- **Discrete**, integer data, e.g. number of points scored, number of people\n",
        "\n",
        "Qualitative data\n",
        "- **Ordinal**, categories with an order, e.g. high, medium, low income\n",
        "- **Nominal**, categories with no order, e.g. name, favorite ice cream\n"
      ],
      "metadata": {
        "id": "rLrDo_ircXBy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "As a data scientist, you might know a lot about programming and statistics and have an area of specialty, but you often are asked to use your skills to solve a problem outside of your domain. One of the key skills you need to develop is the ability to explore a dataset so you can get more context about a particular domain.\n",
        "\n",
        "## Summarizing the data\n",
        "- Descriptive statistics\n",
        "- Plotting"
      ],
      "metadata": {
        "id": "SUfPAYLQuy2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seaborn is a popular data visualization library built with matplotlib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "sns.set_context(\"notebook\")\n",
        "sns.set_theme(style=\"ticks\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "VGVIqqf5uy2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the example dataset for Anscombe's quartet https://en.wikipedia.org/wiki/Anscombe%27s_quartet\n",
        "df = sns.load_dataset(\"anscombe\")\n",
        "\n",
        "# let's check out the mean of both variables in each dataset with groupby (more later...)\n",
        "df.groupby(\"dataset\").mean()"
      ],
      "metadata": {
        "id": "6LeFYJlBTYmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the results of a linear regression within each dataset\n",
        "sns.lmplot(\n",
        "    data=df, x=\"x\", y=\"y\", col=\"dataset\", hue=\"dataset\",\n",
        "    col_wrap=2, palette=\"muted\", ci=None,\n",
        "    height=4, scatter_kws={\"s\": 50, \"alpha\": 1}\n",
        ")"
      ],
      "metadata": {
        "id": "hS2bVNO3TPZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data exploration with penguins dataset\n",
        "\n",
        "First steps with any dataset\n",
        "- How much data is there?\n",
        "- What variables are in the dataset?\n",
        "- What types are the data?\n",
        "- Is any data missing?"
      ],
      "metadata": {
        "id": "8uFjbDAFWQfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset, like in any programming exercise, choose meaningful variable names!\n",
        "penguins = sns.load_dataset(\"penguins\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "WsWNPyRMuy2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how many rows and columns of data do we have? use df.shape attribute\n",
        "penguins.shape"
      ],
      "metadata": {
        "id": "TTBTsnOGV9gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.info() method tells you useful metadata about the data types. what do you notice?\n",
        "penguins.info()"
      ],
      "metadata": {
        "id": "EpT2TEa4WAJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you just want types you can use dtypes\n",
        "# what types are the different variables?\n",
        "penguins.dtypes"
      ],
      "metadata": {
        "id": "JTr4u7rpbwP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What's _Object_? Let's look at the first data point and find out. Warning, object columns may have mixed types!"
      ],
      "metadata": {
        "id": "1bOGVVF1uy2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "penguins.head(1)"
      ],
      "metadata": {
        "id": "TpXkFOa5flpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate descriptive statistics\n",
        "- count\n",
        "- mean: average measurement for whole sample\n",
        "- standard deviation: average deviation from the mean\n",
        "- min/max: highest and lowest values in the sample\n",
        "- percentiles - cutoff values for percent of data when in order, e.g. 75% percentile means 75% of the data is less than this value"
      ],
      "metadata": {
        "id": "XcFfEBgja-R8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use df.describe() to get descriptive statistics on numerical variables - categorical data doesn't show up here\n",
        "penguins.describe()"
      ],
      "metadata": {
        "id": "fxoojGVKVcwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate some summary statistics and look at groups\n",
        "### Group By\n",
        "\n",
        "Group by will help you answer the vast majority of simple data analysis questions. The basic idea is that you group your data by the values of a variable or set of variables, then calculate a statistic of interest like the mean or minimum.\n",
        "\n",
        "https://pandas.pydata.org/docs/user_guide/groupby.html"
      ],
      "metadata": {
        "id": "CKJn3Re8uy2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "penguins.species.unique()"
      ],
      "metadata": {
        "id": "Ytq0ZjfXtdLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "penguins.island.unique()"
      ],
      "metadata": {
        "id": "rkGj17qrtfQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mean of each feature for each group\n",
        "penguins.groupby(\"species\").mean()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "TiwJiVL9uy2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# standard deviation of each feature for each group\n",
        "penguins.groupby(\"species\").std()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "RlOSAvVLuy2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# how correlated are our variables? \n",
        "penguins.corr()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "RLhmEc6Ruy2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Visualization with Seaborn\n",
        "You should try to make visualizations that will help you understand the data:\n",
        "- **Histogram** shows how a single variable is distributed across a range\n",
        "- **Scatter Plot** shows how individual points of data are distributed\n",
        "- **Box Plot** "
      ],
      "metadata": {
        "id": "WQyOP3o_uy2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram\n",
        "sns.histplot(x =\"body_mass_g\", data=penguins)\n",
        "plt.title(\"Body Mass\", size=10)"
      ],
      "metadata": {
        "id": "v1fa9bbyro5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# histogram with categories by species using 'hue' argument\n",
        "sns.histplot(x =\"flipper_length_mm\", data=penguins, hue=\"species\")\n",
        "plt.title(\"Flipper Length\", size=20)"
      ],
      "metadata": {
        "id": "N_EXVGq6sIVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# bar plots are useful for comparing counts and sums or averages, default is average\n",
        "sns.barplot(x=\"species\", y=\"flipper_length_mm\", data=penguins)\n",
        "plt.title(\"Flipper Length for 3 Penguin Species\", size=12)"
      ],
      "metadata": {
        "id": "bqtODdx_tSxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# boxplots show you the distribution of values\n",
        "sns.boxplot(data=penguins, x=\"species\", y=\"flipper_length_mm\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "ZaTUVMaYuy2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# violin plots are like box plots, but using the shape of the distribution instead of a box\n",
        "sns.violinplot(x=\"species\", y=\"body_mass_g\", data=penguins, hue=\"sex\")\n",
        "plt.title(\"Flipper Length for 3 Penguin Species by Sex\", size=12)"
      ],
      "metadata": {
        "id": "qndsa3Raty2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=\"flipper_length_mm\", y=\"body_mass_g\", hue=\"species\", data=penguins)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "azC0u7-yuy2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.scatterplot(x=\"flipper_length_mm\", y=\"body_mass_g\", hue=\"island\", data=penguins)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "px-TExYTuy2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a correlation heatmap, notice that the variable pair for the scatter plot above has a high correlation of 0.87\n",
        "sns.heatmap(penguins.corr(), annot = True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "bzdic0u2uy2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pairplot looks at pairs of variables, can be useful as a first step\n",
        "sns.pairplot(penguins, hue = \"species\", height=2)"
      ],
      "metadata": {
        "id": "a3tusjX5MLA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Missing Data\n",
        "\n",
        "Some machine learning and statistical methods cannot handle missing data. Generally you have two choices for handling this:\n",
        "- Drop missing data\n",
        "- Impute missing data\n",
        "\n",
        "Dropping data can be OK if it's only a small proportion of the overall dataset and/or there are other similar rows with complete data. Dropping data that has otherwise useful information can bias your model and analysis.\n",
        "\n",
        "Imputing data means inserting a substitute value for the missing data. Common methods are using the mean/median for the variable. You can use group by to impute based on a group if you have that data available. More sophisticated methods can use machine learning to impute missing data. Imputation, like dropping data, can result in biased models and analysis if not done carefully.\n",
        "\n",
        "Imputation can require trial and error and there is an art to it, it's also imperfect and you need to think about how it might affect the overall analysis."
      ],
      "metadata": {
        "id": "z61Y3yVQDb16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do we have missing data?\n",
        "penguins.isnull().sum()"
      ],
      "metadata": {
        "id": "iK50Xd6pENxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's impute the median for the species for the continuous variables using fillna\n",
        "penguins[\"bill_length_mm\"].fillna(penguins[\"bill_length_mm\"].median(), inplace=True)\n",
        "penguins[\"bill_depth_mm\"].fillna(penguins[\"bill_depth_mm\"].median(), inplace=True)\n",
        "penguins[\"flipper_length_mm\"].fillna(penguins[\"flipper_length_mm\"].median(), inplace=True)\n",
        "penguins[\"body_mass_g\"].fillna(penguins[\"body_mass_g\"].median(), inplace=True)"
      ],
      "metadata": {
        "id": "aXLmefK5EjeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's impute the missing sex information, first let's make a plot to see if this makes sense with body mass\n",
        "sns.histplot(x=\"body_mass_g\", hue=\"sex\", data=penguins)"
      ],
      "metadata": {
        "id": "5803oZXnFj2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "penguins.groupby(\"sex\").median()"
      ],
      "metadata": {
        "id": "Y5983d5kGtdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the rows with missing sex\n",
        "penguins.loc[penguins[\"sex\"].isnull(),]"
      ],
      "metadata": {
        "id": "yonPTKlAHUlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's use median body mass to impute the missing sex data\n",
        "penguins.body_mass_g.median()"
      ],
      "metadata": {
        "id": "VFHPfR0iH8vB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we use a boolean index to get the rows that are less than the median\n",
        "penguins.loc[penguins[\"sex\"].isnull() & (penguins[\"body_mass_g\"] < penguins.body_mass_g.median())]"
      ],
      "metadata": {
        "id": "wHuiVFiVIvmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this isn't a perfect approach, but since we are doing aggregate analysis, it shouldn't affect the result much\n",
        "penguins[\"sex\"].loc[penguins[\"sex\"].isnull() & (penguins[\"body_mass_g\"] < penguins.body_mass_g.median())] = \"Female\"\n",
        "penguins[\"sex\"].loc[penguins[\"sex\"].isnull() & (penguins[\"body_mass_g\"] >= penguins.body_mass_g.median())] = \"Male\""
      ],
      "metadata": {
        "id": "fsjHT6rHJXuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure there's no more missing data!\n",
        "penguins.isnull().sum()"
      ],
      "metadata": {
        "id": "0wlWFb6aKfvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning: Train/Test Split\n",
        "\n",
        "The biggest difference between descriptive statistics and predictive modeling is that the latter seeks to find a generalizable model that will be good at predicting unseen examples. So our goal isn't just to describe the data, it's to find a pattern that works on new/unseen examples.\n",
        "\n",
        "Overfitting is when your model finds patterns that are specific to your training data and fail to generalize on new examples. For instance, if I asked everyone in the room their favorite pizza topping, I could build a model that associates name to pizza topping. Like if your name is Sam and your favorite topping is pepperoni, I could build a model that says:\n",
        "\n",
        "`if name == \"Sam\":\n",
        "  return \"Pepperoni\"`\n",
        "\n",
        "But this wouldn't be a very good model.\n",
        "\n",
        "In order to combat overfitting, when we train a model we want to hold back some of our data for testing. This is called a train/test split. If our model performs well on the test data, then we can feel confident we didn't overfit.\n",
        "\n",
        "## Training on Time Series data\n",
        "\n",
        "When you are training a model on time series data, it is VERY important to not use dates from the future in your training set. For example, if your dataset has data from 2010-2019, you would want to train on 2010-2017 and test on 2018-2019. There's no perfect rule for picking a data to split on, but whatever you do don't randomly sample the whole dataset!\n",
        "\n",
        "## Choosing an error function\n",
        "We will be evaluating the regression models using Mean Squared Error = AVERAGE(Prediction - True)^2 and R^2 (explained variance)."
      ],
      "metadata": {
        "id": "W8Lsvxlyuy2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning: Classification\n",
        "\n",
        "So now that we have an idea of what the data looks like, let's try to build a model! The most important part of being a professional data scientist is to make sure your model is solving the right problem. Here we can imagine someone discovering a new penguin and not knowing what species it is. We can build a model that can predict the species given the measurements of the penguin!"
      ],
      "metadata": {
        "id": "wRe22Fqcuy2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "APcGdWE_uy2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check no missing data\n",
        "assert penguins.isnull().sum().max() == 0"
      ],
      "metadata": {
        "id": "e4ZNO8fNDX0G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = penguins[\"species\"].copy()\n",
        "\n",
        "# logistic regression works with numerical variables, so we dropped island\n",
        "X = penguins[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]].copy()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(len(X_train), \"Train Instances +\", len(X_test), \"Test Instances\")\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=500).fit(X_train, y_train)\n",
        "\n",
        "# how did we do? score here is accuracy, or correct cases/total cases\n",
        "print(f\"Training set accuracy: {clf.score(X_train, y_train):2f}\")\n",
        "\n",
        "# score on test set\n",
        "print(f\"Test set accuracy: {clf.score(X_test, y_test):2f}\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "s2AeBA6Wuy2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# make predictions\n",
        "predictions = clf.predict(X_test)\n",
        "\n",
        "# Confusion Matrix\n",
        "c_matrix = confusion_matrix(predictions, y_test)\n",
        "print(c_matrix)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "7C-KQmiVuy2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Draw the heatmap for the confusion matrix\n",
        "sns.heatmap(c_matrix, annot=True, square=True)\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "jfBfggqFuy2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning: Regression\n",
        "\n",
        "Regression models involve making a prediction for a continuous (or almost continuous) variable. Things like temperature, price, number of people watching the Super Bowl, etc... Let's try to predict a continuous value from the penguins dataset!"
      ],
      "metadata": {
        "id": "t7t9sqdWuy2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Check\n",
        "* What types are the variables?\n",
        "* Do we have any missing data?"
      ],
      "metadata": {
        "id": "DZ5w1_SWuy2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "y_reg = penguins[\"body_mass_g\"].copy()\n",
        "X_reg = penguins[[\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]].copy()\n",
        "\n",
        "mass_X_train, mass_X_test, mass_y_train, mass_y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "7VAtyKhSuy2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression"
      ],
      "metadata": {
        "id": "KtV2ycTnuy2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create linear regression object\n",
        "regr = LinearRegression()\n",
        "\n",
        "# Train the model using the training sets\n",
        "regr.fit(mass_X_train, mass_y_train)\n",
        "\n",
        "# Make predictions using the testing set\n",
        "mass_y_pred = regr.predict(mass_X_test)\n",
        "\n",
        "# The coefficients\n",
        "print(\"COEFFICIENTS:\")\n",
        "for coef in zip(mass_X_train.columns, regr.coef_):\n",
        "    print(coef[0], \"{:.3f}\".format(coef[1]))\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(mass_y_test, mass_y_pred)))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print('Variance score: {:.2f}'.format(r2_score(mass_y_test, mass_y_pred)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "d2hC49vUuy2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "How do we know if this is a good mean squared error? Let's compare to a simple benchmark: the average of the training data prices:"
      ],
      "metadata": {
        "id": "dxVKCVh5uy2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_mean = mass_y_train.mean()\n",
        "mean_bench = pd.Series([y_train_mean]).repeat(len(mass_y_test))\n",
        "\n",
        "# The mean squared error\n",
        "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(mass_y_test, mean_bench)))\n",
        "# Explained variance score: 1 is perfect prediction - here it's 0 because the \"prediction\" doesn't vary\n",
        "print('Variance score: {:.2f}'.format(r2_score(mass_y_test, mean_bench)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "R4ObAToBuy2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool, so we beat the simplest possible model.\n",
        "\n",
        "Let's compare to a more sophisticated machine learning model:"
      ],
      "metadata": {
        "id": "9vnPVqRXuy2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "_GFEXs57uy2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "KohcsHtfuy2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# one COOL thing about random forest is it can handle categorical and continuous data together\n",
        "# this means we can add 'island' to our list of X variables but we need to convert it to integers\n",
        "\n",
        "# we will use pandas factorize() to do this\n",
        "X_rf_reg = penguins[[\"island\", \"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\"]].copy()\n",
        "X_rf_reg[\"island\"] = X_rf_reg[\"island\"].factorize()[0]\n",
        "\n",
        "# train test split again with same random seed to get same split\n",
        "mass_X_train, mass_X_test, mass_y_train, mass_y_test = train_test_split(X_rf_reg, y_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_regr = RandomForestRegressor(\n",
        "  # We are minimizing MSE\n",
        "  criterion='squared_error',\n",
        "  # Bootstrap\n",
        "  bootstrap=True,\n",
        "  # How deep is each tree in the forest?\n",
        "  max_depth=4,\n",
        "  # How many trees are in the forest?\n",
        "  n_estimators=200,\n",
        "  # Set a random seed so we can reproduce the result\n",
        "  random_state=0,\n",
        "  # Do we want to print information to the console?\n",
        "  verbose=0 #2 YES\n",
        ")\n",
        "\n",
        "rf_regr.fit(mass_X_train, mass_y_train)  \n",
        "\n",
        "\n",
        "#criterion='mse', max_depth=2,\n",
        "#           max_features='auto', max_leaf_nodes=None,\n",
        "#           min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "#           min_samples_leaf=1, min_samples_split=2,\n",
        "#           min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
        "#           oob_score=False, random_state=0, verbose=0, warm_start=False)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "bqhik0rquy2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the testing set\n",
        "y_pred_rf = rf_regr.predict(mass_X_test)\n",
        "\n",
        "# The coefficients\n",
        "print(\"Feature Importances:\")\n",
        "for coef in zip(mass_X_train.columns, rf_regr.feature_importances_):\n",
        "    print(coef[0], \"{:.3f}\".format(coef[1]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "CxIo-NQGuy2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The mean squared error\n",
        "print(\"Mean squared error: {:.2f}\".format(mean_squared_error(mass_y_test, y_pred_rf)))\n",
        "# Explained variance score: 1 is perfect prediction\n",
        "print('Variance score: {:.2f}'.format(r2_score(mass_y_test, y_pred_rf)))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "sN3CygQ6uy2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compare the models!\n",
        "\n",
        "model_results = pd.DataFrame(\n",
        "  {\"model_name\": [\"linear\", \"random forest\", \"mean benchmark\"],\n",
        "    \"r2\": [r2_score(mass_y_test, mass_y_pred),\n",
        "          r2_score(mass_y_test, y_pred_rf),\n",
        "          r2_score(mass_y_test, mean_bench)],\n",
        "  \"mse\":\n",
        "  [mean_squared_error(mass_y_test, mass_y_pred),\n",
        "  mean_squared_error(mass_y_test, y_pred_rf),\n",
        "  mean_squared_error(mass_y_test, mean_bench)]\n",
        "  })"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "_D_9rCojuy2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We are looking for low error and high R^2"
      ],
      "metadata": {
        "id": "Gw2ZFHCJuy2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x=\"model_name\", y=\"mse\", data=model_results)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "-mRc7uXSuy2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise: Plot the r2 for your models!"
      ],
      "metadata": {
        "id": "Nzwx14Uwuy2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x=\"model_name\", y=\"r2\", data=model_results)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "IrT-QvNnuy2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Looks like Random Forest performed the best!"
      ],
      "metadata": {
        "id": "DyaJ5LHVuy2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "1. Choose a public dataset from the provided list or find one of your own. Choose a data set that is in CSV or JSON format if possible. We have limited time, so you will benefit from choosing a clean data set.\n",
        "2. Load the data into a Google Colab notebook\n",
        "3. Explore the data with Pandas and Seaborn\n",
        "4. Look for patterns using descriptive statistics and plots\n",
        "5. Generate an analysis to share with the class that answers the following:\n",
        "* What data is included in your dataset?\n",
        "* What issues or challenges, if any, did you find with the data?\n",
        "* What did you find out so far?\n",
        "* What would you like to explore further?\n",
        "* You can use plots and/or statistics\n",
        "\n",
        "You can present a Colab or Google Slides presentation, feel free to copy code from the Colab we have used today\n",
        "\n",
        "## List of public datasets\n",
        "- [Seaborn data](https://github.com/mwaskom/seaborn-data)\n",
        "- [Data.gov - City of New York](https://catalog.data.gov/organization/city-of-new-york?page=1)\n",
        "- [NYC Health and Environment](https://a816-dohbesp.nyc.gov/IndicatorPublic/beta/)\n",
        "- [NYC Open Data](https://opendata.cityofnewyork.us/data/)\n",
        "- [CDC Places 2022](https://chronicdata.cdc.gov/500-Cities-Places/PLACES-Local-Data-for-Better-Health-County-Data-20/swc5-untb)\n",
        "\n",
        "## Helpful links\n",
        "- Loading files in colab: https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92\n",
        "- Example analysis: https://a816-dohbesp.nyc.gov/IndicatorPublic/beta/data-stories/redlining/"
      ],
      "metadata": {
        "id": "G1CedqIquy2L"
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "nteract": {
      "version": "0.14.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
